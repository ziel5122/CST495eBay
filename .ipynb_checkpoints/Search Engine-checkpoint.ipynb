{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I began by familiarizing myself with the Reuters dataset. If you don't have the data, use nltk.download() from a Python module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<CategorizedPlaintextCorpusReader in u'/home/austin/nltk_data/corpora/reuters.zip/reuters/'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.corpus.reuters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10788"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nltk.corpus.reuters.fileids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test/14826', 'test/14828', 'test/14829', 'test/14832', 'test/14833']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.reuters.fileids()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test/14826', 'test/14828', 'test/14829', 'test/14832', 'test/14833']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import reuters\n",
    "reuters.fileids()[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each document has \"categories\" which I can treat as special information later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'acq', u'alum', u'barley', u'bop', u'carcass', u'castor-oil', u'cocoa', u'coconut', u'coconut-oil', u'coffee', u'copper', u'copra-cake', u'corn', u'cotton', u'cotton-oil', u'cpi', u'cpu', u'crude', u'dfl', u'dlr', u'dmk', u'earn', u'fuel', u'gas', u'gnp', u'gold', u'grain', u'groundnut', u'groundnut-oil', u'heat', u'hog', u'housing', u'income', u'instal-debt', u'interest', u'ipi', u'iron-steel', u'jet', u'jobs', u'l-cattle', u'lead', u'lei', u'lin-oil', u'livestock', u'lumber', u'meal-feed', u'money-fx', u'money-supply', u'naphtha', u'nat-gas', u'nickel', u'nkr', u'nzdlr', u'oat', u'oilseed', u'orange', u'palladium', u'palm-oil', u'palmkernel', u'pet-chem', u'platinum', u'potato', u'propane', u'rand', u'rape-oil', u'rapeseed', u'reserves', u'retail', u'rice', u'rubber', u'rye', u'ship', u'silver', u'sorghum', u'soy-meal', u'soy-oil', u'soybean', u'strategic-metal', u'sugar', u'sun-meal', u'sun-oil', u'sunseed', u'tea', u'tin', u'trade', u'veg-oil', u'wheat', u'wpi', u'yen', u'zinc']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[u'coffee', u'lumber', u'palm-oil', u'rubber', u'veg-oil']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print reuters.categories()\n",
    "\n",
    "test_article = reuters.fileids()[6]\n",
    "reuters.categories(test_article)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The beginning of each article has a title in all capital letters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'INDONESIAN', u'COMMODITY', u'EXCHANGE', u'MAY', u'EXPAND', u'The']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reuters.words(test_article)[:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'd like to experiment with weighing these tokens heavier than the rest of the corpus later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "Before doing anything fancy, I should clean up the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'trade']\n",
      "[u'ASIAN', u'EXPORTERS', u'FEAR', u'DAMAGE', u'FROM', u'U', u'.', u'S', u'.-', u'JAPAN'] \n",
      "\n",
      "['trade']\n",
      "['asian', 'exporters', 'fear', 'damage', 'from', 'u', '.', 's', '.-', 'japan']\n"
     ]
    }
   ],
   "source": [
    "print reuters.categories(reuters.fileids()[0])\n",
    "print reuters.words(reuters.fileids()[0])[:10], \"\\n\"\n",
    "\n",
    "categories = []\n",
    "documents = []\n",
    "\n",
    "for file_id in reuters.fileids():\n",
    "    temp = []\n",
    "    for category in reuters.categories(file_id):\n",
    "        temp.append(category.encode('utf-8'))\n",
    "    categories.append(temp)\n",
    "    \n",
    "    temp = []\n",
    "    for word in reuters.words(file_id):\n",
    "        temp.append(word.encode('utf-8').lower())\n",
    "    documents.append(temp)\n",
    "\n",
    "print categories[0]\n",
    "print documents[0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm beginning with a very simple model intentionally - ignoring case, ignoring all punctuation, etc. - so that I can add onto it and see if/how things improve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Building the Engine\n",
    "\n",
    "The next step is to create the inverted index. I'm omitting categories for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 : 2\n",
      "3333 : 1\n",
      "6758 : 1\n",
      "1546 : 1\n",
      "4183 : 1\n"
     ]
    }
   ],
   "source": [
    "def createInvertedIndex(documents):\n",
    "    idx = {}\n",
    "    \n",
    "    for i,document in enumerate(documents):\n",
    "        for word in document:\n",
    "            if word in idx:\n",
    "                if i in idx[word]:\n",
    "                    idx[word][i] += 1\n",
    "                else:\n",
    "                    idx[word][i] = 1\n",
    "            else:\n",
    "                idx[word] = {i:1}\n",
    "    return idx\n",
    "        \n",
    "idx = createInvertedIndex(documents)\n",
    "for i,pair in enumerate(idx['thailand']):\n",
    "    if i < 5:\n",
    "        print pair,':',idx['thailand'][pair]\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "It's then possible to evaluate my first search query with term frequency as the only information metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 from recall set of 88 items:\n",
      "\t5.00 - ['brazil', 'says', 'debt', 'crisis', 'is', 'world', 'problem', 'brazilian'] length: 420\n",
      "\t4.00 - ['portuguese', 'economy', 'remains', 'buoyant', 'despite', 'crisis', 'portugal', \"'\"] length: 583\n",
      "\t4.00 - ['papandreou', 'shows', '\"', 'restricted', 'optimism', '\"', 'over', 'crisis'] length: 307\n",
      "\t3.00 - ['treasury', \"'\", 's', 'baker', 'under', 'fire', 'for', 'wall'] length: 871\n",
      "\t3.00 - ['cash', 'crisis', 'hits', 'ugandan', 'coffee', 'board', 'uganda', \"'\"] length: 556\n",
      "\t2.00 - ['tropical', 'forest', 'death', 'could', 'spark', 'new', 'debt', 'crisis'] length: 651\n",
      "\t2.00 - ['ec', 'commission', 'given', 'plan', 'to', 'save', 'steel', 'industry'] length: 339\n",
      "\t2.00 - ['ex', '-', 'arco', '&', 'lt', ';', 'arc', '>'] length: 292\n",
      "\t2.00 - ['diplomats', 'call', 'u', '.', 's', '.', 'attack', 'on'] length: 560\n",
      "\t2.00 - ['oecd', 'trade', ',', 'growth', 'seen', 'slowing', 'in', '1987'] length: 763\n"
     ]
    }
   ],
   "source": [
    "def searchTF(query, corpus, idx):\n",
    "    scores = {}\n",
    "    \n",
    "    for word in query.split():\n",
    "        for doc_num in idx[word]:\n",
    "            if doc_num in scores:\n",
    "                scores[doc_num] += idx[word][doc_num]\n",
    "            else:\n",
    "                scores[doc_num] = idx[word][doc_num]\n",
    "    \n",
    "    results = []\n",
    "    for pair in [[score[0],score[1]] for score in zip(scores.keys(), scores.values())]:\n",
    "        results.append([pair[1], pair[0]])\n",
    "        \n",
    "    return sorted(results, key=lambda x: x[0] * -1)\n",
    "    \n",
    "\n",
    "def printResults(results, corpus, n, head=True):\n",
    "    ''' Helper function to print results\n",
    "    '''\n",
    "    if head:    \n",
    "        print('\\nTop %d from recall set of %d items:' % (n,len(results)))\n",
    "        for r in results[:n]:\n",
    "            print('\\t%0.2f - %s length: %d'%(r[0],corpus[r[1]][:8],len(corpus[r[1]])))\n",
    "    else:\n",
    "        print('\\nBottom %d from recall set of %d items:' % (n,len(results)))\n",
    "        for r in results[-n:]:\n",
    "            print('\\t%0.2f - %s length: %d'%(r[0],corpus[r[1]][:8],len(corpus[r[1]])))\n",
    "\n",
    "idx = createInvertedIndex(documents)\n",
    "scores = searchTF('hostage crisis', documents, idx)\n",
    "printResults(scores, documents, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting document set is less than spectacular. The 9th query looks promising to be an actual article on a hostage crisis, but it, like the others, lacks 'hostage'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print 'hostage' in documents[scores[8][1]]\n",
    "print 'crisis' in documents[scores[8][1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 from recall set of 1 items:\n",
      "\t1.00 - ['unocal', '&', 'lt', ';', 'ucl', '>', 'plans'] length: 435\n"
     ]
    }
   ],
   "source": [
    "scores = searchTF('hostage', documents, idx)\n",
    "printResults(scores, documents, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe the query isn't the most fair since only one document in the entire corpus contains 'hostage'. I'll try another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 from recall set of 5351 items:\n",
      "\t69.00 - ['economic', 'spotlight', '-', 'u', '.', 's', '.'] length: 1202\n",
      "\t53.00 - ['coffee', 'talks', 'failure', 'seen', 'pressuring', 'u', '.'] length: 848\n",
      "\t50.00 - ['asian', 'exporters', 'fear', 'damage', 'from', 'u', '.'] length: 899\n",
      "\t47.00 - ['japan', 'rejects', 'u', '.', 's', '.', 'objections'] length: 623\n",
      "\t46.00 - ['japan', 'ministry', 'says', 'open', 'farm', 'trade', 'would'] length: 589\n",
      "\t45.00 - ['china', 'calls', 'for', 'better', 'trade', 'deal', 'with'] length: 911\n",
      "\t45.00 - ['ample', 'supplies', 'limit', 'u', '.', 's', '.'] length: 694\n",
      "\t40.00 - ['louvre', 'accord', 'still', 'in', 'effect', ',', 'japan'] length: 791\n",
      "\t37.00 - ['february', 'u', '.', 's', '.', 'jobs', 'gains'] length: 1019\n",
      "\t37.00 - ['fairchild', 'deal', 'failure', 'seen', 'making', 'japanese', 'wary'] length: 817\n"
     ]
    }
   ],
   "source": [
    "scores = searchTF('u s national debt', documents, idx)\n",
    "printResults(scores, documents, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are noticeably better. One thing that stands out, however, is that longer queries seem to dominate the top, and the weighting of all tokens is the same, when really they should be diminished by their prevalence.\n",
    "\n",
    "## Improving the Results\n",
    "\n",
    "First, we can change the weights of terms by implementing IDF to our score, then later on, adjust the document scores by their length. In eyeballing the results, US seems to appear more which is good; it's weight has likely beein increased significantly by the addition of IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 from recall set of 5351 items:\n",
      "\t74.23 - ['economic', 'spotlight', '-', 'u', '.', 's', '.', 'deficit'] length: 1202\n",
      "\t62.30 - ['u', '.', 's', '.', 'urges', 'banks', 'to', 'weigh'] length: 983\n",
      "\t61.77 - ['coffee', 'talks', 'failure', 'seen', 'pressuring', 'u', '.', 's'] length: 848\n",
      "\t52.59 - ['japan', 'rejects', 'u', '.', 's', '.', 'objections', 'to'] length: 623\n",
      "\t52.15 - ['asian', 'exporters', 'fear', 'damage', 'from', 'u', '.', 's'] length: 899\n",
      "\t51.14 - ['japan', 'ministry', 'says', 'open', 'farm', 'trade', 'would', 'hit'] length: 589\n",
      "\t48.96 - ['ample', 'supplies', 'limit', 'u', '.', 's', '.', 'strike'] length: 694\n",
      "\t48.26 - ['china', 'calls', 'for', 'better', 'trade', 'deal', 'with', 'u'] length: 911\n",
      "\t46.81 - ['offer', 'for', 'dome', 'may', 'short', '-', 'circuit', 'its'] length: 536\n",
      "\t46.81 - ['offer', 'for', 'dome', 'may', 'short', '-', 'circuit', 'its'] length: 536\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def idfCalc(term, idx, n):\n",
    "    return math.log( float(n) / (1 + len(idx[term])))\n",
    "\n",
    "def searchTFIDF(query, corpus, idx):\n",
    "    scores = {}\n",
    "    \n",
    "    for word in query.split():\n",
    "        if word in idx:\n",
    "            idf = idfCalc(word, idx, len(corpus))\n",
    "            \n",
    "            for doc_num in idx[word]:\n",
    "                if doc_num in scores:\n",
    "                    scores[doc_num] += idx[word][doc_num] * idf\n",
    "                else:\n",
    "                    scores[doc_num] = idx[word][doc_num] * idf\n",
    "    \n",
    "    results = []\n",
    "    for pair in [[score[0],score[1]] for score in zip(scores.keys(), scores.values())]:\n",
    "        results.append([pair[1], pair[0]])\n",
    "        \n",
    "    return sorted(results, key=lambda x: x[0] * -1)\n",
    "\n",
    "scores = searchTFIDF('u s national debt', documents, idx)\n",
    "printResults(scores, documents, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to do something about the document length. I'll start by simply dividing the score by the length of the document. This should give me a sort of relevance per token metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 from recall set of 5351 items:\n",
      "\t0.36 - ['swiss', 'national', 'bank', 'says', 'bought', 'dollars', 'against', 'yen'] length: 16\n",
      "\t0.32 - ['viacom', 'international', 'inc', 'gets', 'another', 'new', 'national', 'amusements'] length: 18\n",
      "\t0.26 - ['viacom', 'said', 'it', 'has', 'new', 'national', 'amusements', ','] length: 22\n",
      "\t0.26 - ['beneficial', 'corp', 'to', 'sell', 'western', 'national', 'life', 'for'] length: 22\n",
      "\t0.26 - ['union', 'national', '&', 'lt', ';', 'unbc', '>', 'signs'] length: 70\n",
      "\t0.25 - ['u', '.', 's', '.', 'video', '&', 'lt', ';'] length: 84\n",
      "\t0.25 - ['key', 'u', '.', 's', '.', 'house', 'trade', 'subcommittee'] length: 36\n",
      "\t0.23 - ['yeutter', 'says', 'u', '.', 's', '.', 'should', 'stress'] length: 40\n",
      "\t0.21 - ['security', 'pacific', ',', 'provident', 'national', 'lift', 'prime', 'security'] length: 41\n",
      "\t0.21 - ['abbey', 'national', 'said', 'it', 'cutting', 'u', '.', 'k'] length: 42\n"
     ]
    }
   ],
   "source": [
    "def searchTFIDFNorm(query, corpus, idx):\n",
    "    scores = {}\n",
    "    \n",
    "    for word in query.split():\n",
    "        docs = []\n",
    "        if word in idx:\n",
    "            idf = idfCalc(word, idx, len(corpus))\n",
    "            \n",
    "            for doc_num in idx[word]:\n",
    "                if doc_num in scores:\n",
    "                    scores[doc_num] += idx[word][doc_num] * idf\n",
    "                else:\n",
    "                    scores[doc_num] = idx[word][doc_num] * idf\n",
    "    \n",
    "    results = []\n",
    "    for pair in [[score[0],score[1]] for score in zip(scores.keys(), scores.values())]:\n",
    "        results.append([pair[1] / len(corpus[pair[0]]), pair[0]])\n",
    "        \n",
    "    return sorted(results, key=lambda x: x[0] * -1)\n",
    "\n",
    "scores = searchTFIDFNorm('u s national debt', documents, idx)\n",
    "printResults(scores, documents, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results appear to be disastrous. Very short queries are winning our because occurences of 'national' for example aren't being mitigated very heavily by a document of length 16. Perhaps a more moderate approach like BM25 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
